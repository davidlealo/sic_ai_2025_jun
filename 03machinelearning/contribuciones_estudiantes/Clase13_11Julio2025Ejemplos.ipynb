{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# pag 378 Algoritmos de ensamble\n",
        "\n",
        "Algoritmos de Conjunto\n"
      ],
      "metadata": {
        "id": "2ZuVyjVzokRg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "se tienen distintos modelos y cada unoi da su opinion, y luegop se ¬øvota? para ver cual es la mejor respuesta?"
      ],
      "metadata": {
        "id": "OmjojYeaong3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ‚úÖ ¬øQu√© son los **Algoritmos de Conjunto** (Ensemble Learning)?\n",
        "\n",
        "Son **t√©cnicas de aprendizaje autom√°tico** que combinan **varios modelos individuales** (por ejemplo, varios √°rboles de decisi√≥n, o varios clasificadores) para obtener **una predicci√≥n final m√°s precisa y robusta**.\n",
        "\n",
        "La idea es que **muchos modelos simples, cuando trabajan juntos, pueden hacerlo mejor que un solo modelo complejo.**\n",
        "\n",
        "---\n",
        "\n",
        "### ü§ñ ¬øC√≥mo funciona eso de que \"cada modelo da su opini√≥n\"?\n",
        "\n",
        "Supongamos que queremos saber si una imagen es de un **gato** o de un **perro**. Tenemos 3 modelos distintos:\n",
        "\n",
        "* **Modelo 1** dice: \"Es un gato\"\n",
        "* **Modelo 2** dice: \"Es un perro\"\n",
        "* **Modelo 3** dice: \"Es un gato\"\n",
        "\n",
        "Aqu√≠, cada modelo da **su opini√≥n o predicci√≥n**.\n",
        "\n",
        "---\n",
        "\n",
        "### üó≥Ô∏è ¬øY c√≥mo se elige la respuesta final? (¬ø\"votar\"?)\n",
        "\n",
        "¬°Exactamente! Lo que se hace es una **votaci√≥n** (en modelos de clasificaci√≥n). Existen varias formas de combinar las opiniones, pero las m√°s comunes son:\n",
        "\n",
        "#### 1. **Votaci√≥n mayoritaria (Majority Voting)**\n",
        "\n",
        "Se elige la opci√≥n que recibi√≥ m√°s votos.\n",
        "\n",
        "> En el ejemplo anterior, 2 modelos dijeron ‚Äúgato‚Äù y 1 dijo ‚Äúperro‚Äù, as√≠ que la respuesta final es: **gato**.\n",
        "\n",
        "#### 2. **Promedio (en regresi√≥n)**\n",
        "\n",
        "Cuando se trabaja con valores num√©ricos (regresi√≥n), se puede hacer el **promedio de las salidas** de los modelos.\n",
        "\n",
        "---\n",
        "\n",
        "### üéØ ¬øPor qu√© es √∫til usar algoritmos de conjunto?\n",
        "\n",
        "* **Reduce el error**: Disminuye el riesgo de que un mal modelo arruine la predicci√≥n.\n",
        "* **Mejora la generalizaci√≥n**: Se comporta mejor con datos nuevos.\n",
        "* **Disminuye el sobreajuste** (cuando un modelo aprende demasiado bien los datos de entrenamiento y falla en los nuevos).\n",
        "\n",
        "---\n",
        "\n",
        "### üß† Ejemplos conocidos de algoritmos de conjunto:\n",
        "\n",
        "| T√©cnica      | Descripci√≥n breve                                                                                                  |\n",
        "| ------------ | ------------------------------------------------------------------------------------------------------------------ |\n",
        "| **Bagging**  | Usa muchos modelos entrenados con diferentes subconjuntos del conjunto de datos. Ejemplo: **Random Forest**.       |\n",
        "| **Boosting** | Los modelos se entrenan secuencialmente, corrigiendo los errores del anterior. Ejemplo: **AdaBoost**, **XGBoost**. |\n",
        "| **Stacking** | Combina las predicciones de varios modelos mediante otro modelo que aprende a combinar sus salidas.                |\n",
        "\n",
        "---\n",
        "\n",
        "### üìù En resumen\n",
        "\n",
        "S√≠, en cierto modo **cada modelo \"da su opini√≥n\"**, y luego **se combinan todas esas opiniones** (como una votaci√≥n o promedio) para tomar una decisi√≥n final. Esto permite que el sistema **sea m√°s confiable** que depender solo de un modelo.\n",
        "\n"
      ],
      "metadata": {
        "id": "sqFiK1Z_pQFK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "En los algoritmos de conjunto, como `VotingClassifier`, existen dos formas principales de combinar los resultados de varios modelos:\n",
        "\n",
        "---\n",
        "\n",
        "## üî∑ 1. **Votaci√≥n Dura (Hard Voting)**\n",
        "\n",
        "üëâ Cada modelo **vota por una clase** (como si dijera \"yo creo que es esto\") y gana la clase que **reciba m√°s votos**.\n",
        "\n",
        "### üó≥Ô∏è Ejemplo:\n",
        "\n",
        "Supongamos que tenemos 3 modelos que predicen si un pasajero sobrevivi√≥ (`1`) o no (`0`) en el Titanic:\n",
        "\n",
        "* Modelo A: **1**\n",
        "* Modelo B: **0**\n",
        "* Modelo C: **1**\n",
        "\n",
        "‚úÖ Resultado final (por mayor√≠a): **1**\n",
        "\n",
        "**No importa qu√© tan confiado est√© cada modelo**, solo cuenta su voto.\n",
        "\n",
        "### üìå Caracter√≠sticas:\n",
        "\n",
        "* Sencillo y r√°pido.\n",
        "* Funciona bien si todos los modelos tienen una precisi√≥n razonable.\n",
        "* No considera la ‚Äúconfianza‚Äù del modelo en su predicci√≥n.\n",
        "\n",
        "---\n",
        "\n",
        "## üî∑ 2. **Votaci√≥n Suave o Blanda (Soft Voting)**\n",
        "\n",
        "üëâ Cada modelo no solo da su predicci√≥n, sino tambi√©n **la probabilidad** de cada clase. Se suman esas probabilidades y se elige la clase con **mayor suma total**.\n",
        "\n",
        "### üé≤ Ejemplo:\n",
        "\n",
        "Supongamos que tres modelos devuelven estas probabilidades para cada clase:\n",
        "\n",
        "| Modelo | Probabilidad de 0 | Probabilidad de 1 |\n",
        "| ------ | ----------------- | ----------------- |\n",
        "| A      | 0.3               | 0.7               |\n",
        "| B      | 0.6               | 0.4               |\n",
        "| C      | 0.2               | 0.8               |\n",
        "\n",
        "üîç Suma de probabilidades:\n",
        "\n",
        "* Clase 0: **0.3 + 0.6 + 0.2 = 1.1**\n",
        "* Clase 1: **0.7 + 0.4 + 0.8 = 1.9**\n",
        "\n",
        "‚úÖ Resultado final: **1** (tiene mayor probabilidad total)\n",
        "\n",
        "### üìå Caracter√≠sticas:\n",
        "\n",
        "* M√°s precisa si los modelos pueden calcular probabilidades (como regresi√≥n log√≠stica).\n",
        "* Usa informaci√≥n m√°s rica (la confianza del modelo).\n",
        "* Requiere que todos los modelos tengan el m√©todo `.predict_proba()`.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚öôÔ∏è ¬øC√≥mo se configura esto en Python con `VotingClassifier`?\n",
        "\n",
        "```python\n",
        "# Hard voting (por defecto)\n",
        "VotingClassifier(estimators=[...], voting='hard')\n",
        "\n",
        "# Soft voting (requiere modelos que tengan predict_proba)\n",
        "VotingClassifier(estimators=[...], voting='soft')\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ ¬øCu√°l usar?\n",
        "\n",
        "| Situaci√≥n                                | Recomendaci√≥n          |\n",
        "| ---------------------------------------- | ---------------------- |\n",
        "| Modelos que no tienen `.predict_proba()` | Usa **votaci√≥n dura**  |\n",
        "| Modelos con buenas probabilidades        | Usa **votaci√≥n suave** |\n",
        "| Buscas algo r√°pido y sencillo            | Dura                   |\n",
        "| Quieres precisi√≥n m√°s fina               | Suave                  |\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "bv3K1stFqBt2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ejemplo con Titanic!!"
      ],
      "metadata": {
        "id": "9hIfxOe3prCm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install seaborn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c1TmVjjop1Be",
        "outputId": "01de516b-ed8b-40a7-f7f5-4135087ec1d1"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (0.13.2)\n",
            "Requirement already satisfied: numpy!=1.24.0,>=1.20 in /usr/local/lib/python3.11/dist-packages (from seaborn) (2.0.2)\n",
            "Requirement already satisfied: pandas>=1.2 in /usr/local/lib/python3.11/dist-packages (from seaborn) (2.2.2)\n",
            "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in /usr/local/lib/python3.11/dist-packages (from seaborn) (3.10.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.58.5)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2->seaborn) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2->seaborn) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Paso 1: Importar librer√≠as\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Paso 2: Importar modelos\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "# Modelos para ejemplo Bikes\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.ensemble import RandomForestRegressor, VotingRegressor\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.datasets import fetch_openml\n",
        "from math import sqrt\n",
        "\n",
        "# Modelos para ejemplo california_housing\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "\n"
      ],
      "metadata": {
        "id": "PCSG965SqWHT"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Paso 3: Cargar dataset del Titanic desde seaborn\n",
        "titanic = sns.load_dataset(\"titanic\")\n",
        "\n",
        "# Paso 4: Preprocesamiento simple\n",
        "# Seleccionamos algunas columnas relevantes\n",
        "df = titanic[[\"survived\", \"pclass\", \"sex\", \"age\", \"fare\", \"embarked\"]].copy()\n",
        "\n",
        "# Eliminamos filas con valores nulos en 'embarked'\n",
        "df = df.dropna(subset=[\"embarked\"])\n",
        "\n",
        "# Imputamos valores faltantes de 'age' con la media\n",
        "imputer = SimpleImputer(strategy=\"mean\")\n",
        "df[\"age\"] = imputer.fit_transform(df[[\"age\"]])\n",
        "\n",
        "# Codificamos las variables categ√≥ricas\n",
        "df[\"sex\"] = LabelEncoder().fit_transform(df[\"sex\"])\n",
        "df[\"embarked\"] = LabelEncoder().fit_transform(df[\"embarked\"])\n",
        "\n",
        "# Paso 5: Dividir en X (caracter√≠sticas) e y (etiqueta)\n",
        "X = df.drop(\"survived\", axis=1)\n",
        "y = df[\"survived\"]"
      ],
      "metadata": {
        "id": "_ojmbo1cqgQl"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dividir en entrenamiento y prueba\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "HIM68hPUqjYJ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Paso 6: Crear modelos individuales\n",
        "model_knn = KNeighborsClassifier(n_neighbors=5)\n",
        "model_tree = DecisionTreeClassifier(random_state=42)\n",
        "model_logreg = LogisticRegression(max_iter=1000)"
      ],
      "metadata": {
        "id": "8PbOW1cmqr4a"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ZYJj0y-oLWy",
        "outputId": "1348c470-ca58-481a-b850-c86e6c919dc7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precisi√≥n del modelo de conjunto (VotingClassifier): 0.8034\n"
          ]
        }
      ],
      "source": [
        "# Paso 7: Crear el modelo de conjunto (votaci√≥n)\n",
        "voting_clf = VotingClassifier(\n",
        "    estimators=[\n",
        "        ('knn', model_knn),\n",
        "        ('tree', model_tree),\n",
        "        ('logreg', model_logreg)\n",
        "    ],\n",
        "    voting='hard'  # votaci√≥n mayoritaria\n",
        ")\n",
        "\n",
        "# Entrenar el modelo\n",
        "voting_clf.fit(X_train, y_train)\n",
        "\n",
        "# Predecir y evaluar\n",
        "y_pred = voting_clf.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Precisi√≥n del modelo de conjunto (VotingClassifier): {accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ahora con SOFT\n",
        "\n",
        "# Crear modelos que s√≠ permiten .predict_proba()\n",
        "model_logreg = LogisticRegression(max_iter=1000)\n",
        "model_tree = DecisionTreeClassifier(random_state=42)\n",
        "model_nb = GaussianNB()"
      ],
      "metadata": {
        "id": "ZerQ356YrK_a"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# VotingClassifier con votaci√≥n SUAVE (soft)\n",
        "voting_soft = VotingClassifier(\n",
        "    estimators=[\n",
        "        ('logreg', model_logreg),\n",
        "        ('tree', model_tree),\n",
        "        ('nb', model_nb)\n",
        "    ],\n",
        "    voting='soft'  # <--- aqui se cambia a suave\n",
        ")\n",
        "\n",
        "# Entrenar y evaluar\n",
        "voting_soft.fit(X_train, y_train)\n",
        "y_pred_soft = voting_soft.predict(X_test)\n",
        "accuracy_soft = accuracy_score(y_test, y_pred_soft)\n",
        "\n",
        "print(f\"Precisi√≥n con votaci√≥n SUAVE: {accuracy_soft:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "76NiDYCiqSNi",
        "outputId": "f8530245-d5c8-4230-c752-ffe45ca3e212"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precisi√≥n con votaci√≥n SUAVE: 0.8146\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Comparaci√≥n con votaci√≥n DURA\n",
        "voting_hard = VotingClassifier(\n",
        "    estimators=[\n",
        "        ('logreg', model_logreg),\n",
        "        ('tree', model_tree),\n",
        "        ('nb', model_nb)\n",
        "    ],\n",
        "    voting='hard'  # Aqu√≠ usamos votaci√≥n dura\n",
        ")\n",
        "\n",
        "voting_hard.fit(X_train, y_train)\n",
        "y_pred_hard = voting_hard.predict(X_test)\n",
        "accuracy_hard = accuracy_score(y_test, y_pred_hard)\n",
        "\n",
        "print(f\"Precisi√≥n con votaci√≥n DURA:  {accuracy_hard:.4f}\")\n",
        "print(f\"Precisi√≥n con votaci√≥n SUAVE: {accuracy_soft:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ta3SbfnFrfT1",
        "outputId": "b34f1bc0-b3ff-431c-a63b-4dad9e3ac9a3"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precisi√≥n con votaci√≥n DURA:  0.7809\n",
            "Precisi√≥n con votaci√≥n SUAVE: 0.8146\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ejemplo con Bikes/Bicicletas!!\n",
        "\n",
        "en este se le agregaran pesos a los modelos en votingRegressor"
      ],
      "metadata": {
        "id": "bjRXNLpW1suS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cargar datos\n",
        "bike = fetch_openml(name=\"Bike_Sharing_Demand\", version=2, as_frame=True)\n",
        "df = bike.frame\n",
        "\n",
        "# Preprocesamiento\n",
        "df['holiday'] = df['holiday'].astype(str).str.lower().map({'false': 0, 'true': 1})\n",
        "df['workingday'] = df['workingday'].astype(str).str.lower().map({'false': 0, 'true': 1})\n",
        "df['weekday'] = df['weekday'].astype(int)\n",
        "\n",
        "le_season = LabelEncoder()\n",
        "le_weather = LabelEncoder()\n",
        "df['season'] = le_season.fit_transform(df['season'])\n",
        "df['weather'] = le_weather.fit_transform(df['weather'])\n",
        "\n",
        "# Variables predictoras y objetivo\n",
        "X = df[['season', 'hour', 'holiday', 'weekday', 'workingday', 'weather', 'temp', 'feel_temp', 'humidity', 'windspeed']]\n",
        "y = df['count']\n",
        "\n",
        "# Escalado\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Divisi√≥n de datos\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.25, random_state=42)\n",
        "\n"
      ],
      "metadata": {
        "id": "nTIulx5mrkiF"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f5L771DrrpN_",
        "outputId": "59d00a96-248b-4f6d-c690-4470c021df62"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Modelo: Regresi√≥n Lineal\n",
            "R¬≤: 0.349\n",
            "RMSE: 144.69\n",
            "\n",
            "Modelo: √Årbol de Decisi√≥n\n",
            "R¬≤: 0.716\n",
            "RMSE: 95.51\n",
            "\n",
            "Modelo: KNN\n",
            "R¬≤: 0.630\n",
            "RMSE: 109.02\n",
            "\n",
            "Modelo: SVR\n",
            "R¬≤: 0.353\n",
            "RMSE: 144.19\n",
            "\n",
            "Modelo: SVM (lineal)\n",
            "R¬≤: 0.305\n",
            "RMSE: 149.51\n",
            "\n",
            "Modelo: Voting Regressor\n",
            "R¬≤: 0.775\n",
            "RMSE: 85.11\n"
          ]
        }
      ],
      "source": [
        "# Modelos individuales\n",
        "modelos = {\n",
        "    \"Regresi√≥n Lineal\": LinearRegression(),\n",
        "    \"√Årbol de Decisi√≥n\": DecisionTreeRegressor(random_state=42),\n",
        "    \"KNN\": KNeighborsRegressor(n_neighbors=5),\n",
        "    \"SVR\": SVR(),\n",
        "    \"SVM (lineal)\": SVR(kernel='linear')\n",
        "}\n",
        "\n",
        "# Entrenar y evaluar modelos individuales\n",
        "for nombre, modelo in modelos.items():\n",
        "    modelo.fit(X_train, y_train)\n",
        "    y_pred = modelo.predict(X_test)\n",
        "    print(f\"\\nModelo: {nombre}\")\n",
        "    print(f\"R¬≤: {r2_score(y_test, y_pred):.3f}\")\n",
        "    print(f\"RMSE: {sqrt(mean_squared_error(y_test, y_pred)):.2f}\")\n",
        "\n",
        "# Voting Regressor con pesos\n",
        "voting = VotingRegressor([\n",
        "    ('lr', LinearRegression()),\n",
        "    ('rf', RandomForestRegressor(n_estimators=100, random_state=42)),\n",
        "    ('knn', KNeighborsRegressor(n_neighbors=5))\n",
        "], weights=[2,3,1])\n",
        "\n",
        "voting.fit(X_train, y_train)\n",
        "y_pred_voting = voting.predict(X_test)\n",
        "print(f\"\\nModelo: Voting Regressor\")\n",
        "print(f\"R¬≤: {r2_score(y_test, y_pred_voting):.3f}\")\n",
        "print(f\"RMSE: {sqrt(mean_squared_error(y_test, y_pred_voting)):.2f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ejemplo california_housing"
      ],
      "metadata": {
        "id": "y5CjDie23y3F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cargar datos\n",
        "data = fetch_california_housing()\n",
        "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "\n",
        "# Variables predictoras y objetivo\n",
        "X = df[['MedInc', 'AveRooms']]\n",
        "y = data.target\n",
        "\n",
        "# Divisi√≥n de datos\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
        "\n",
        "# Escalado\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Modelos individuales\n",
        "lr = LinearRegression().fit(X_train, y_train)\n",
        "tree = DecisionTreeRegressor(max_depth=5).fit(X_train, y_train)\n",
        "knn = KNeighborsRegressor(n_neighbors=5).fit(X_train_scaled, y_train)\n",
        "svm = SVR().fit(X_train_scaled, y_train)\n",
        "\n",
        "# Crear muestra para predicci√≥n\n",
        "sample = pd.DataFrame([{\n",
        "    'MedInc': 3.5,\n",
        "    'AveRooms': 4\n",
        "}])\n",
        "\n",
        "# Predicciones individuales\n",
        "pred_lr = lr.predict(sample)\n",
        "pred_tree = tree.predict(sample)\n",
        "pred_knn = knn.predict(scaler.transform(sample))\n",
        "pred_svm = svm.predict(scaler.transform(sample))\n",
        "\n",
        "print(\"Regresi√≥n Lineal:\", round(pred_lr[0] * 100000, 2), \"USD\")\n",
        "print(\"√Årbol de Decisi√≥n:\", round(pred_tree[0] * 100000, 2), \"USD\")\n",
        "print(\"KNN:\", round(pred_knn[0] * 100000, 2), \"USD\")\n",
        "print(\"SVR:\", round(pred_svm[0] * 100000, 2), \"USD\")\n",
        "\n",
        "# === VotingRegressor con mismos modelos con pesos ===\n",
        "voting = VotingRegressor([\n",
        "    ('lr', LinearRegression()),\n",
        "    ('tree', DecisionTreeRegressor(max_depth=5)),\n",
        "    ('knn', KNeighborsRegressor(n_neighbors=5)),\n",
        "    ('svr', SVR())\n",
        "], weights=[2,2,1,3])\n",
        "\n",
        "# Entrenamiento con datos escalados\n",
        "voting.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Predicci√≥n para muestra\n",
        "pred_voting = voting.predict(scaler.transform(sample))\n",
        "print(\"Voting Regressor:\", round(pred_voting[0] * 100000, 2), \"USD\")\n",
        "\n",
        "# Evaluaci√≥n VotingRegressor\n",
        "y_pred_voting = voting.predict(X_test_scaled)\n",
        "print(\"\\nEvaluaci√≥n Voting Regressor:\")\n",
        "print(f\"R¬≤: {r2_score(y_test, y_pred_voting):.3f}\")\n",
        "print(f\"RMSE: {np.sqrt(mean_squared_error(y_test, y_pred_voting)):.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gGEZgqQ24EH4",
        "outputId": "46bc7a41-29ed-49da-8dc2-2aa9aad55198"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Regresi√≥n Lineal: 196104.97 USD\n",
            "√Årbol de Decisi√≥n: 232202.21 USD\n",
            "KNN: 292800.2 USD\n",
            "SVR: 221607.22 USD\n",
            "Voting Regressor: 226779.53 USD\n",
            "\n",
            "Evaluaci√≥n Voting Regressor:\n",
            "R¬≤: 0.529\n",
            "RMSE: 0.79\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vAsIE-FI5ndR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}